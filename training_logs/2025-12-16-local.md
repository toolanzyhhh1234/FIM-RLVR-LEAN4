(base) admin1@DESKTOP-EV45IB9:~/CodeProjects/FIM-RLVR-LEAN4$ python train_grpo_fim.py
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Unsloth: Could not import trl.trainer.alignprop_trainer: Failed to import trl.trainer.alignprop_trainer because of the following error (look up to see its traceback):
cannot import name 'DDPOStableDiffusionPipeline' from 'trl.models' (/home/admin1/miniconda3/lib/python3.13/site-packages/trl/models/__init__.py)
Loading model: unsloth/Qwen2.5-0.5B-Instruct
==((====))==  Unsloth 2025.12.5: Fast Qwen2 patching. Transformers: 4.57.3.
   \\   /|    NVIDIA GeForce RTX 3070. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth 2025.12.5 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.
Loading dataset from data/fim_fresh.jsonl
Preprocessing dataset...
Unsloth: We now expect `per_device_train_batch_size` * `gradient_accumulation_steps` * `world_size` to be a multiple of `num_generations`.
We will change the batch size of 1 to the `num_generations` of 4
Starting training...
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 50
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4
 "-____-"     Trainable parameters = 2,199,552 of 496,232,320 (0.44% trained)
  0%|                                                                                      | 0/50 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.0, 'grad_norm': 0.39149314165115356, 'learning_rate': 0.0, 'num_tokens': 1436.0, 'completions/mean_length': 170.0, 'completions/min_length': 62.0, 'completions/max_length': 282.0, 'completions/clipped_ratio': 0.0, 'completions/mean_terminated_length': 170.0, 'completions/min_terminated_length': 62.0, 'completions/max_terminated_length': 282.0, 'rewards/format_reward/mean': 0.875, 'rewards/format_reward/std': 0.25, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.875, 'reward_std': 0.25, 'frac_reward_zero_std': 0.0, 'completion_length': 282.0, 'kl': 0.0, 'epoch': 0.0}
{'loss': -0.0, 'grad_norm': 0.35806748270988464, 'learning_rate': 1e-05, 'num_tokens': 4473.0, 'completions/mean_length': 394.25, 'completions/min_length': 135.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 276.5, 'completions/min_terminated_length': 135.0, 'completions/max_terminated_length': 418.0, 'rewards/format_reward/mean': 0.875, 'rewards/format_reward/std': 0.25, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.875, 'reward_std': 0.25, 'frac_reward_zero_std': 0.0, 'completion_length': 512.0, 'kl': 0.0, 'epoch': 0.0}
  4%|â–ˆâ–ˆâ–ˆ                                                                           | 2/50 [01:12<27:50, 34.81s/it]Unsloth: Input IDs of shape torch.Size([4, 1252]) with length 1252 > the model's max sequence length of 1024.
We shall truncate it ourselves. It's imperative if you correct this issue first.
{'loss': 0.0, 'grad_norm': 6.743995189666748, 'learning_rate': 2e-05, 'num_tokens': 9481.0, 'completions/mean_length': 512.0, 'completions/min_length': 512.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/format_reward/mean': 0.875, 'rewards/format_reward/std': 0.25, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.875, 'reward_std': 0.25, 'frac_reward_zero_std': 0.0, 'completion_length': 512.0, 'kl': 0.012163832783699036, 'epoch': 0.0}
  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                         | 3/50 [01:48<27:34, 35.19s/it]Unsloth: Input IDs of shape torch.Size([4, 1367]) with length 1367 > the model's max sequence length of 1024.
We shall truncate it ourselves. It's imperative if you correct this issue first.
{'loss': 0.0, 'grad_norm': 0.0005955594824627042, 'learning_rate': 3e-05, 'num_tokens': 14949.0, 'completions/mean_length': 512.0, 'completions/min_length': 512.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/format_reward/mean': 1.0, 'rewards/format_reward/std': 0.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 512.0, 'kl': 0.009690267033874989, 'epoch': 0.0}
{'loss': 0.0, 'grad_norm': 0.4466346502304077, 'learning_rate': 4e-05, 'num_tokens': 16948.0, 'completions/mean_length': 126.75, 'completions/min_length': 2.0, 'completions/max_length': 276.0, 'completions/clipped_ratio': 0.0, 'completions/mean_terminated_length': 126.75, 'completions/min_terminated_length': 2.0, 'completions/max_terminated_length': 276.0, 'rewards/format_reward/mean': 0.75, 'rewards/format_reward/std': 0.28867512941360474, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.75, 'reward_std': 0.28867512941360474, 'frac_reward_zero_std': 0.0, 'completion_length': 276.0, 'kl': 0.0014081861590966582, 'epoch': 0.01}
{'loss': 0.0, 'grad_norm': 0.30935361981391907, 'learning_rate': 5e-05, 'num_tokens': 19399.0, 'completions/mean_length': 331.75, 'completions/min_length': 179.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.25, 'completions/mean_terminated_length': 271.66668701171875, 'completions/min_terminated_length': 179.0, 'completions/max_terminated_length': 341.0, 'rewards/format_reward/mean': 0.625, 'rewards/format_reward/std': 0.25, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.625, 'reward_std': 0.25, 'frac_reward_zero_std': 0.0, 'completion_length': 512.0, 'kl': 0.0010300016729161143, 'epoch': 0.01}
 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                    | 6/50 [02:59<19:23, 26.44s/it]Unsloth: Input IDs of shape torch.Size([4, 1536]) with length 1536 > the model's max sequence length of 1024.
We shall truncate it ourselves. It's imperative if you correct this issue first.
{'loss': 0.0, 'grad_norm': 0.0003452312375884503, 'learning_rate': 4.888888888888889e-05, 'num_tokens': 25540.0, 'completions/mean_length': 511.25, 'completions/min_length': 509.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.75, 'completions/mean_terminated_length': 509.0, 'completions/min_terminated_length': 509.0, 'completions/max_terminated_length': 509.0, 'rewards/format_reward/mean': 1.0, 'rewards/format_reward/std': 0.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 512.0, 'kl': 0.007700036279857159, 'epoch': 0.01}
{'loss': 0.0, 'grad_norm': 5.359062194824219, 'learning_rate': 4.7777777777777784e-05, 'num_tokens': 31310.0, 'completions/mean_length': 418.5, 'completions/min_length': 138.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.75, 'completions/mean_terminated_length': 138.0, 'completions/min_terminated_length': 138.0, 'completions/max_terminated_length': 138.0, 'rewards/format_reward/mean': 0.875, 'rewards/format_reward/std': 0.25, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.875, 'reward_std': 0.25, 'frac_reward_zero_std': 0.0, 'completion_length': 512.0, 'kl': 0.00791039876639843, 'epoch': 0.01}
{'loss': 0.0, 'grad_norm': 0.0007458853651769459, 'learning_rate': 4.666666666666667e-05, 'num_tokens': 37288.0, 'completions/mean_length': 470.5, 'completions/min_length': 346.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.75, 'completions/mean_terminated_length': 346.0, 'completions/min_terminated_length': 346.0, 'completions/max_terminated_length': 346.0, 'rewards/format_reward/mean': 1.0, 'rewards/format_reward/std': 0.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 512.0, 'kl': 0.008538627997040749, 'epoch': 0.01}
{'loss': 0.0, 'grad_norm': 0.0005306036910042167, 'learning_rate': 4.555555555555556e-05, 'num_tokens': 43432.0, 'completions/mean_length': 512.0, 'completions/min_length': 512.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 1.0, 'completions/mean_terminated_length': 0.0, 'completions/min_terminated_length': 0.0, 'completions/max_terminated_length': 0.0, 'rewards/format_reward/mean': 1.0, 'rewards/format_reward/std': 0.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 512.0, 'kl': 0.0077813914977014065, 'epoch': 0.01}
{'loss': 0.0, 'grad_norm': 0.0010943362722173333, 'learning_rate': 4.4444444444444447e-05, 'num_tokens': 47140.0, 'completions/mean_length': 450.0, 'completions/min_length': 384.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 388.0, 'completions/min_terminated_length': 384.0, 'completions/max_terminated_length': 392.0, 'rewards/format_reward/mean': 1.0, 'rewards/format_reward/std': 0.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 512.0, 'kl': 0.004321233369410038, 'epoch': 0.01}
 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                            | 11/50 [05:17<17:45, 27.32s/it]Unsloth: Input IDs of shape torch.Size([4, 1034]) with length 1034 > the model's max sequence length of 1024.
We shall truncate it ourselves. It's imperative if you correct this issue first.
{'loss': 0.0, 'grad_norm': 6.291593551635742, 'learning_rate': 4.3333333333333334e-05, 'num_tokens': 50768.0, 'completions/mean_length': 385.0, 'completions/min_length': 21.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 258.0, 'completions/min_terminated_length': 21.0, 'completions/max_terminated_length': 495.0, 'rewards/format_reward/mean': 0.875, 'rewards/format_reward/std': 0.25, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.875, 'reward_std': 0.25, 'frac_reward_zero_std': 0.0, 'completion_length': 512.0, 'kl': 0.0223329309374094, 'epoch': 0.01}
{'loss': 0.0, 'grad_norm': 0.00041293876711279154, 'learning_rate': 4.222222222222222e-05, 'num_tokens': 52804.0, 'completions/mean_length': 330.0, 'completions/min_length': 148.0, 'completions/max_length': 512.0, 'completions/clipped_ratio': 0.5, 'completions/mean_terminated_length': 148.0, 'completions/min_terminated_length': 148.0, 'completions/max_terminated_length': 148.0, 'rewards/format_reward/mean': 1.0, 'rewards/format_reward/std': 0.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 1.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 512.0, 'kl': 0.0017731478437781334, 'epoch': 0.01}
 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                         | 13/50 [06:27<18:53, 30.63s/it]