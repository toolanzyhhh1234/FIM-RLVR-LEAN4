ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
Unsloth: UnslothBCOTrainer is already patched.
Unsloth: UnslothCPOTrainer is already patched.
Unsloth: UnslothDPOTrainer is already patched.
Unsloth: UnslothGKDTrainer is already patched.
Unsloth: UnslothGRPOTrainer is already patched.
Unsloth: UnslothKTOTrainer is already patched.
Unsloth: UnslothNashMDTrainer is already patched.
Unsloth: UnslothOnlineDPOTrainer is already patched.
Unsloth: UnslothORPOTrainer is already patched.
Unsloth: UnslothPPOTrainer is already patched.
Unsloth: UnslothPRMTrainer is already patched.
Unsloth: UnslothRewardTrainer is already patched.
Unsloth: UnslothRLOOTrainer is already patched.
Unsloth: UnslothSFTTrainer is already patched.
Unsloth: UnslothXPOTrainer is already patched.
Loading model: unsloth/gpt-oss-20b
==((====))==  Unsloth 2025.12.5: Fast Gpt_Oss patching. Transformers: 4.57.3.
   \\   /|    GRID A100D-40C. Num GPUs = 1. Max memory: 39.996 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:11,  3.99s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:11<00:12,  6.11s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:17<00:06,  6.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:18<00:00,  3.95s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:18<00:00,  4.60s/it]
Unsloth: Making `model.base_model.model.model` require gradients
Loading dataset from data/data/train-00000-of-00001.parquet
Dataset rows (no filtering applied): 104155
Setting up dynamic curriculum transform...
Unsloth: We now expect `per_device_train_batch_size` * `gradient_accumulation_steps` * `world_size` to be a multiple of `num_generations`.
We will change the batch size of 1 to the `num_generations` of 2
Starting training with Curriculum...
The model is already on multiple devices. Skipping the move to device specified in `args`.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 104,155 | Num Epochs = 1 | Total steps = 2
O^O/ \_/ \    Batch size per device = 2 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2
 "-____-"     Trainable parameters = 7,962,624 of 20,922,719,808 (0.04% trained)
  0%|          | 0/2 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'max_length': 131072}. If this is not desired, please set these values explicitly.
Unsloth: Will smartly offload gradients to save VRAM!
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [12:16<12:16, 736.34s/it]                                              {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'num_tokens': 2152.0, 'completions/mean_length': 961.0, 'completions/min_length': 579.0, 'completions/max_length': 1343.0, 'completions/clipped_ratio': 0.0, 'completions/mean_terminated_length': 961.0, 'completions/min_terminated_length': 579.0, 'completions/max_terminated_length': 1343.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 961.0, 'kl': 0.0, 'epoch': 0.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [12:16<12:16, 736.34s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [19:25<00:00, 555.71s/it]                                              {'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5e-05, 'num_tokens': 3573.0, 'completions/mean_length': 595.5, 'completions/min_length': 405.0, 'completions/max_length': 786.0, 'completions/clipped_ratio': 0.0, 'completions/mean_terminated_length': 595.5, 'completions/min_terminated_length': 405.0, 'completions/max_terminated_length': 786.0, 'rewards/lean_validity_reward/mean': 0.0, 'rewards/lean_validity_reward/std': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'frac_reward_zero_std': 1.0, 'completion_length': 595.5, 'kl': 0.0, 'epoch': 0.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [19:25<00:00, 555.71s/it]Saving curriculum state to outputs_fim_grpo/curriculum_state.json
Saving LoRA adapters to outputs_fim_grpo
                                              {'train_runtime': 1172.0914, 'train_samples_per_second': 0.003, 'train_steps_per_second': 0.002, 'train_loss': 0.0, 'epoch': 0.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [19:32<00:00, 555.71s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [19:32<00:00, 586.04s/it]
Training finished.
Saving curriculum state to outputs_fim_grpo/curriculum_state.json
Saving final LoRA adapters to outputs_fim_grpo/final_adapters
