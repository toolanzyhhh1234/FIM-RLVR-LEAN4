+ export GPUS_PER_NODE=1
+ GPUS_PER_NODE=1
+ export NNODES=1
+ NNODES=1
+ export TORCH_CUDA_ARCH_LIST=12.0+PTX
+ TORCH_CUDA_ARCH_LIST=12.0+PTX
+ export VLLM_USE_V1=1
+ VLLM_USE_V1=1
+ export WANDB_MODE=disabled
+ WANDB_MODE=disabled
+ project_name=RL-GSPO
+ adv_estimator=grpo
+ loss_mode=gspo
+ loss_agg_mode=seq-mean-token-mean
+ MODEL_PATH=Qwen/Qwen2.5-3B-Instruct
+ offload=false
+ rollout_engine=vllm
+ rollout_mode=async
+ return_raw_chat=True
+ gpu_memory_utilization=0.6
+ reward_manager=dapo
+ shuffle_dataset=true
+ train_batch_size=64
+ ppo_mini_batch_size=16
+ ppo_micro_batch_size_per_gpu=2
+ n_resp_per_prompt=4
+ max_prompt_length=2048
+ max_response_length=4096
+ enable_overlong_buffer=false
+ overlong_buffer_len=4096
+ overlong_penalty_factor=1.0
+ test_freq=10
+ save_freq=10
+ total_epochs=1
+ total_training_steps=100
+ val_before_train=false
+ use_kl_in_reward=false
+ kl_coef=0.0
+ use_kl_loss=false
+ kl_loss_coef=0.0
+ clip_ratio_low=0.0003
+ clip_ratio_high=0.0004
++ basename Qwen/Qwen2.5-3B-Instruct
+ SFT_MODEL=Qwen2.5-3B-Instruct
+ exp_name=gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL
+ CKPTS_DIR=/workspace/verl/checkpoints/gspo/gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL
+ temperature=1.0
+ top_p=1.0
+ top_k=-1
+ val_top_p=0.7
+ sp_size=1
+ use_dynamic_bsz=true
+ actor_ppo_max_token_len=12288
+ infer_ppo_max_token_len=18432
+ gen_tp=1
+ entropy_checkpointing=true
+ DATA_DIR=/workspace/verl/data/gsm8k
+ mkdir -p /workspace/verl/data/gsm8k
+ '[' '!' -f /workspace/verl/data/gsm8k/train.parquet ']'
+ python verl/examples/data_preprocess/gsm8k.py --local_save_dir /workspace/verl/data/gsm8k
Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 765309.94 examples/s]
Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 512201.37 examples/s]
Map:   0%|          | 0/7473 [00:00<?, ? examples/s]Map:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 2869/7473 [00:00<00:00, 19996.81 examples/s]Map:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7000/7473 [00:00<00:00, 30616.41 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:00<00:00, 29129.57 examples/s]
Map:   0%|          | 0/1319 [00:00<?, ? examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 40042.90 examples/s]
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.38ba/s]
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 208.82ba/s]
+ gsm8k_train_path=/workspace/verl/data/gsm8k/train.parquet
+ gsm8k_test_path=/workspace/verl/data/gsm8k/test.parquet
+ train_files='['\''/workspace/verl/data/gsm8k/train.parquet'\'']'
+ test_files='['\''/workspace/verl/data/gsm8k/test.parquet'\'']'
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo actor_rollout_ref.actor.policy_loss.loss_mode=gspo +actor_rollout_ref.model.override_config.attn_implementation=sdpa +actor_rollout_ref.ref.override_config.attn_implementation=sdpa 'data.train_files=['\''/workspace/verl/data/gsm8k/train.parquet'\'']' 'data.val_files=['\''/workspace/verl/data/gsm8k/test.parquet'\'']' data.shuffle=true data.prompt_key=prompt data.truncation=error data.filter_overlong_prompts=true data.return_raw_chat=True data.train_batch_size=64 data.max_prompt_length=2048 data.max_response_length=4096 actor_rollout_ref.rollout.n=4 algorithm.use_kl_in_reward=false algorithm.kl_ctrl.kl_coef=0.0 actor_rollout_ref.actor.use_kl_loss=false actor_rollout_ref.actor.kl_loss_coef=0.0 actor_rollout_ref.actor.clip_ratio_low=0.0003 actor_rollout_ref.actor.clip_ratio_high=0.0004 actor_rollout_ref.model.use_remove_padding=false actor_rollout_ref.actor.use_dynamic_bsz=true actor_rollout_ref.ref.log_prob_use_dynamic_bsz=true actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=true actor_rollout_ref.actor.ppo_max_token_len_per_gpu=12288 actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=18432 actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=18432 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.mode=async actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct actor_rollout_ref.model.enable_gradient_checkpointing=true actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.05 actor_rollout_ref.actor.optim.weight_decay=0.1 actor_rollout_ref.actor.ppo_mini_batch_size=16 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2 actor_rollout_ref.actor.fsdp_config.param_offload=false actor_rollout_ref.actor.fsdp_config.optimizer_offload=false actor_rollout_ref.actor.entropy_coeff=0 actor_rollout_ref.actor.grad_clip=1.0 actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-mean actor_rollout_ref.actor.ulysses_sequence_parallel_size=1 actor_rollout_ref.rollout.gpu_memory_utilization=0.6 actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.enable_chunked_prefill=true actor_rollout_ref.rollout.max_num_batched_tokens=6144 actor_rollout_ref.rollout.temperature=1.0 actor_rollout_ref.rollout.top_p=1.0 actor_rollout_ref.rollout.top_k=-1 actor_rollout_ref.rollout.val_kwargs.temperature=1.0 actor_rollout_ref.rollout.val_kwargs.top_p=0.7 actor_rollout_ref.rollout.val_kwargs.top_k=-1 actor_rollout_ref.rollout.val_kwargs.do_sample=true actor_rollout_ref.rollout.val_kwargs.n=1 actor_rollout_ref.ref.fsdp_config.param_offload=false actor_rollout_ref.ref.ulysses_sequence_parallel_size=1 actor_rollout_ref.actor.entropy_checkpointing=true reward_model.reward_manager=dapo +reward_model.reward_kwargs.overlong_buffer_cfg.enable=false +reward_model.reward_kwargs.overlong_buffer_cfg.len=4096 +reward_model.reward_kwargs.overlong_buffer_cfg.penalty_factor=1.0 +reward_model.reward_kwargs.overlong_buffer_cfg.log=false +reward_model.reward_kwargs.max_resp_len=4096 'trainer.logger=["console"]' trainer.project_name=RL-GSPO trainer.experiment_name=gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL trainer.n_gpus_per_node=1 trainer.nnodes=1 trainer.val_before_train=false trainer.test_freq=10 trainer.save_freq=10 trainer.total_epochs=1 trainer.total_training_steps=100 trainer.default_local_dir=/workspace/verl/checkpoints/gspo/gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL trainer.resume_mode=auto trainer.log_val_generations=2
2025-12-24 04:58:35,723	INFO worker.py:1942 -- Started a local Ray instance. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
[36m(pid=5690)[0m W1224 04:58:41.469000 5690 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
ray init kwargs: {'num_cpus': None, 'runtime_env': {'env_vars': {'TOKENIZERS_PARALLELISM': 'true', 'NCCL_DEBUG': 'WARN', 'VLLM_LOGGING_LEVEL': 'WARN', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'true', 'VLLM_ALLREDUCE_USE_SYMM_MEM': '0', 'CUDA_DEVICE_MAX_CONNECTIONS': '1', 'NCCL_CUMEM_ENABLE': '0'}, 'working_dir': None}}
[36m(TaskRunner pid=5690)[0m TaskRunner hostname: verl-related-work, PID: 5690
[36m(TaskRunner pid=5690)[0m {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.FSDPActorConfig',
[36m(TaskRunner pid=5690)[0m                                  'calculate_entropy': False,
[36m(TaskRunner pid=5690)[0m                                  'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=5690)[0m                                                 'async_save': False,
[36m(TaskRunner pid=5690)[0m                                                 'load_contents': ['model',
[36m(TaskRunner pid=5690)[0m                                                                   'optimizer',
[36m(TaskRunner pid=5690)[0m                                                                   'extra'],
[36m(TaskRunner pid=5690)[0m                                                 'save_contents': ['model',
[36m(TaskRunner pid=5690)[0m                                                                   'optimizer',
[36m(TaskRunner pid=5690)[0m                                                                   'extra']},
[36m(TaskRunner pid=5690)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=5690)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=5690)[0m                                  'clip_ratio_high': 0.0004,
[36m(TaskRunner pid=5690)[0m                                  'clip_ratio_low': 0.0003,
[36m(TaskRunner pid=5690)[0m                                  'data_loader_seed': 42,
[36m(TaskRunner pid=5690)[0m                                  'entropy_checkpointing': True,
[36m(TaskRunner pid=5690)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=5690)[0m                                  'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=5690)[0m                                  'freeze_vision_tower': False,
[36m(TaskRunner pid=5690)[0m                                  'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=5690)[0m                                                  'dtype': 'bfloat16',
[36m(TaskRunner pid=5690)[0m                                                  'entropy_checkpointing': False,
[36m(TaskRunner pid=5690)[0m                                                  'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=5690)[0m                                                  'forward_only': False,
[36m(TaskRunner pid=5690)[0m                                                  'forward_prefetch': False,
[36m(TaskRunner pid=5690)[0m                                                  'fsdp_size': -1,
[36m(TaskRunner pid=5690)[0m                                                  'full_determinism': False,
[36m(TaskRunner pid=5690)[0m                                                  'model_dtype': 'fp32',
[36m(TaskRunner pid=5690)[0m                                                  'offload_policy': False,
[36m(TaskRunner pid=5690)[0m                                                  'optimizer_offload': False,
[36m(TaskRunner pid=5690)[0m                                                  'param_offload': False,
[36m(TaskRunner pid=5690)[0m                                                  'reshard_after_forward': True,
[36m(TaskRunner pid=5690)[0m                                                  'seed': 42,
[36m(TaskRunner pid=5690)[0m                                                  'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m                                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                                  'use_orig_params': False,
[36m(TaskRunner pid=5690)[0m                                                  'use_torch_compile': True,
[36m(TaskRunner pid=5690)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=5690)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=5690)[0m                                  'kl_loss_coef': 0.0,
[36m(TaskRunner pid=5690)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=5690)[0m                                  'loss_agg_mode': 'seq-mean-token-mean',
[36m(TaskRunner pid=5690)[0m                                  'loss_scale_factor': None,
[36m(TaskRunner pid=5690)[0m                                  'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig',
[36m(TaskRunner pid=5690)[0m                                            'betas': [0.9, 0.999],
[36m(TaskRunner pid=5690)[0m                                            'clip_grad': 1.0,
[36m(TaskRunner pid=5690)[0m                                            'lr': 1e-06,
[36m(TaskRunner pid=5690)[0m                                            'lr_scheduler_type': 'constant',
[36m(TaskRunner pid=5690)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=5690)[0m                                            'lr_warmup_steps_ratio': 0.05,
[36m(TaskRunner pid=5690)[0m                                            'min_lr_ratio': 0.0,
[36m(TaskRunner pid=5690)[0m                                            'num_cycles': 0.5,
[36m(TaskRunner pid=5690)[0m                                            'optimizer': 'AdamW',
[36m(TaskRunner pid=5690)[0m                                            'optimizer_impl': 'torch.optim',
[36m(TaskRunner pid=5690)[0m                                            'override_optimizer_config': None,
[36m(TaskRunner pid=5690)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=5690)[0m                                            'warmup_style': None,
[36m(TaskRunner pid=5690)[0m                                            'weight_decay': 0.1},
[36m(TaskRunner pid=5690)[0m                                  'policy_loss': {'_target_': 'verl.workers.config.PolicyLossConfig',
[36m(TaskRunner pid=5690)[0m                                                  'clip_cov_lb': 1.0,
[36m(TaskRunner pid=5690)[0m                                                  'clip_cov_ratio': 0.0002,
[36m(TaskRunner pid=5690)[0m                                                  'clip_cov_ub': 5.0,
[36m(TaskRunner pid=5690)[0m                                                  'kl_cov_ratio': 0.0002,
[36m(TaskRunner pid=5690)[0m                                                  'loss_mode': 'gspo',
[36m(TaskRunner pid=5690)[0m                                                  'ppo_kl_coef': 0.1},
[36m(TaskRunner pid=5690)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=5690)[0m                                  'ppo_max_token_len_per_gpu': 12288,
[36m(TaskRunner pid=5690)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=5690)[0m                                  'ppo_micro_batch_size_per_gpu': 2,
[36m(TaskRunner pid=5690)[0m                                  'ppo_mini_batch_size': 16,
[36m(TaskRunner pid=5690)[0m                                  'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=5690)[0m                                               'all_ranks': False,
[36m(TaskRunner pid=5690)[0m                                               'enable': False,
[36m(TaskRunner pid=5690)[0m                                               'ranks': [],
[36m(TaskRunner pid=5690)[0m                                               'save_path': 'outputs/profile',
[36m(TaskRunner pid=5690)[0m                                               'tool': None,
[36m(TaskRunner pid=5690)[0m                                               'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                       'analysis': True,
[36m(TaskRunner pid=5690)[0m                                                                       'contents': [],
[36m(TaskRunner pid=5690)[0m                                                                       'discrete': False,
[36m(TaskRunner pid=5690)[0m                                                                       'level': 'level0'},
[36m(TaskRunner pid=5690)[0m                                                               'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                        'discrete': False},
[36m(TaskRunner pid=5690)[0m                                                               'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                         'step_end': None,
[36m(TaskRunner pid=5690)[0m                                                                         'step_start': 0},
[36m(TaskRunner pid=5690)[0m                                                               'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                                'stack_depth': 32,
[36m(TaskRunner pid=5690)[0m                                                                                'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=5690)[0m                                  'rollout_n': 4,
[36m(TaskRunner pid=5690)[0m                                  'router_replay': {'_target_': 'verl.workers.config.RouterReplayConfig',
[36m(TaskRunner pid=5690)[0m                                                    'mode': 'disabled',
[36m(TaskRunner pid=5690)[0m                                                    'record_file': None,
[36m(TaskRunner pid=5690)[0m                                                    'replay_file': None},
[36m(TaskRunner pid=5690)[0m                                  'shuffle': False,
[36m(TaskRunner pid=5690)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                  'use_dynamic_bsz': True,
[36m(TaskRunner pid=5690)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=5690)[0m                                  'use_kl_loss': False,
[36m(TaskRunner pid=5690)[0m                                  'use_remove_padding': False,
[36m(TaskRunner pid=5690)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=5690)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=5690)[0m                        'model': {'_target_': 'verl.workers.config.HFModelConfig',
[36m(TaskRunner pid=5690)[0m                                  'custom_chat_template': None,
[36m(TaskRunner pid=5690)[0m                                  'enable_activation_offload': False,
[36m(TaskRunner pid=5690)[0m                                  'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=5690)[0m                                  'exclude_modules': None,
[36m(TaskRunner pid=5690)[0m                                  'external_lib': None,
[36m(TaskRunner pid=5690)[0m                                  'fused_kernel_options': {'impl_backend': 'torch'},
[36m(TaskRunner pid=5690)[0m                                  'hf_config_path': None,
[36m(TaskRunner pid=5690)[0m                                  'lora_adapter_path': None,
[36m(TaskRunner pid=5690)[0m                                  'lora_alpha': 16,
[36m(TaskRunner pid=5690)[0m                                  'lora_rank': 0,
[36m(TaskRunner pid=5690)[0m                                  'override_config': {'attn_implementation': 'sdpa'},
[36m(TaskRunner pid=5690)[0m                                  'path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(TaskRunner pid=5690)[0m                                  'target_modules': 'all-linear',
[36m(TaskRunner pid=5690)[0m                                  'tokenizer_path': None,
[36m(TaskRunner pid=5690)[0m                                  'trust_remote_code': False,
[36m(TaskRunner pid=5690)[0m                                  'use_fused_kernels': False,
[36m(TaskRunner pid=5690)[0m                                  'use_liger': False,
[36m(TaskRunner pid=5690)[0m                                  'use_remove_padding': False,
[36m(TaskRunner pid=5690)[0m                                  'use_shm': False},
[36m(TaskRunner pid=5690)[0m                        'nccl_timeout': 600,
[36m(TaskRunner pid=5690)[0m                        'ref': {'_target_': 'verl.workers.config.FSDPActorConfig',
[36m(TaskRunner pid=5690)[0m                                'entropy_checkpointing': False,
[36m(TaskRunner pid=5690)[0m                                'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=5690)[0m                                'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=5690)[0m                                                'dtype': 'bfloat16',
[36m(TaskRunner pid=5690)[0m                                                'entropy_checkpointing': False,
[36m(TaskRunner pid=5690)[0m                                                'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=5690)[0m                                                'forward_only': True,
[36m(TaskRunner pid=5690)[0m                                                'forward_prefetch': False,
[36m(TaskRunner pid=5690)[0m                                                'fsdp_size': -1,
[36m(TaskRunner pid=5690)[0m                                                'full_determinism': False,
[36m(TaskRunner pid=5690)[0m                                                'model_dtype': 'fp32',
[36m(TaskRunner pid=5690)[0m                                                'offload_policy': False,
[36m(TaskRunner pid=5690)[0m                                                'optimizer_offload': False,
[36m(TaskRunner pid=5690)[0m                                                'param_offload': False,
[36m(TaskRunner pid=5690)[0m                                                'reshard_after_forward': True,
[36m(TaskRunner pid=5690)[0m                                                'seed': 42,
[36m(TaskRunner pid=5690)[0m                                                'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m                                                'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                                'use_orig_params': False,
[36m(TaskRunner pid=5690)[0m                                                'use_torch_compile': True,
[36m(TaskRunner pid=5690)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=5690)[0m                                'log_prob_max_token_len_per_gpu': 18432,
[36m(TaskRunner pid=5690)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=5690)[0m                                'log_prob_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=5690)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=5690)[0m                                'override_config': {'attn_implementation': 'sdpa'},
[36m(TaskRunner pid=5690)[0m                                'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=5690)[0m                                             'all_ranks': False,
[36m(TaskRunner pid=5690)[0m                                             'enable': False,
[36m(TaskRunner pid=5690)[0m                                             'ranks': [],
[36m(TaskRunner pid=5690)[0m                                             'save_path': 'outputs/profile',
[36m(TaskRunner pid=5690)[0m                                             'tool': None,
[36m(TaskRunner pid=5690)[0m                                             'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                     'analysis': True,
[36m(TaskRunner pid=5690)[0m                                                                     'contents': [],
[36m(TaskRunner pid=5690)[0m                                                                     'discrete': False,
[36m(TaskRunner pid=5690)[0m                                                                     'level': 'level0'},
[36m(TaskRunner pid=5690)[0m                                                             'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                      'discrete': False},
[36m(TaskRunner pid=5690)[0m                                                             'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                       'step_end': None,
[36m(TaskRunner pid=5690)[0m                                                                       'step_start': 0},
[36m(TaskRunner pid=5690)[0m                                                             'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                              'stack_depth': 32,
[36m(TaskRunner pid=5690)[0m                                                                              'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=5690)[0m                                'rollout_n': 4,
[36m(TaskRunner pid=5690)[0m                                'router_replay': {'_target_': 'verl.workers.config.RouterReplayConfig',
[36m(TaskRunner pid=5690)[0m                                                  'mode': 'disabled',
[36m(TaskRunner pid=5690)[0m                                                  'record_file': None,
[36m(TaskRunner pid=5690)[0m                                                  'replay_file': None},
[36m(TaskRunner pid=5690)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m                                'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                'use_torch_compile': True},
[36m(TaskRunner pid=5690)[0m                        'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=5690)[0m                                    'agent': {'_target_': 'verl.workers.config.AgentLoopConfig',
[36m(TaskRunner pid=5690)[0m                                              'agent_loop_config_path': None,
[36m(TaskRunner pid=5690)[0m                                              'custom_async_server': {'_target_': 'verl.workers.config.CustomAsyncServerConfig',
[36m(TaskRunner pid=5690)[0m                                                                      'name': None,
[36m(TaskRunner pid=5690)[0m                                                                      'path': None},
[36m(TaskRunner pid=5690)[0m                                              'default_agent_loop': 'single_turn_agent',
[36m(TaskRunner pid=5690)[0m                                              'num_workers': 8},
[36m(TaskRunner pid=5690)[0m                                    'calculate_log_probs': False,
[36m(TaskRunner pid=5690)[0m                                    'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=5690)[0m                                    'data_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                    'disable_log_stats': True,
[36m(TaskRunner pid=5690)[0m                                    'do_sample': True,
[36m(TaskRunner pid=5690)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=5690)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=5690)[0m                                    'enable_prefix_caching': True,
[36m(TaskRunner pid=5690)[0m                                    'enable_rollout_routing_replay': False,
[36m(TaskRunner pid=5690)[0m                                    'enforce_eager': False,
[36m(TaskRunner pid=5690)[0m                                    'engine_kwargs': {'sglang': {}, 'vllm': {}},
[36m(TaskRunner pid=5690)[0m                                    'expert_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=5690)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=5690)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=5690)[0m                                    'layered_summon': False,
[36m(TaskRunner pid=5690)[0m                                    'load_format': 'dummy',
[36m(TaskRunner pid=5690)[0m                                    'log_prob_max_token_len_per_gpu': 18432,
[36m(TaskRunner pid=5690)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=5690)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=5690)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(TaskRunner pid=5690)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=5690)[0m                                    'max_num_batched_tokens': 6144,
[36m(TaskRunner pid=5690)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=5690)[0m                                    'mode': 'async',
[36m(TaskRunner pid=5690)[0m                                    'multi_stage_wake_up': False,
[36m(TaskRunner pid=5690)[0m                                    'multi_turn': {'_target_': 'verl.workers.config.MultiTurnConfig',
[36m(TaskRunner pid=5690)[0m                                                   'enable': False,
[36m(TaskRunner pid=5690)[0m                                                   'format': 'hermes',
[36m(TaskRunner pid=5690)[0m                                                   'interaction_config_path': None,
[36m(TaskRunner pid=5690)[0m                                                   'max_assistant_turns': None,
[36m(TaskRunner pid=5690)[0m                                                   'max_parallel_calls': 1,
[36m(TaskRunner pid=5690)[0m                                                   'max_tool_response_length': 256,
[36m(TaskRunner pid=5690)[0m                                                   'max_user_turns': None,
[36m(TaskRunner pid=5690)[0m                                                   'num_repeat_rollouts': None,
[36m(TaskRunner pid=5690)[0m                                                   'tokenization_sanity_check_mode': 'strict',
[36m(TaskRunner pid=5690)[0m                                                   'tool_config_path': None,
[36m(TaskRunner pid=5690)[0m                                                   'tool_response_truncate_side': 'middle',
[36m(TaskRunner pid=5690)[0m                                                   'use_inference_chat_template': False},
[36m(TaskRunner pid=5690)[0m                                    'n': 4,
[36m(TaskRunner pid=5690)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=5690)[0m                                    'over_sample_rate': 0,
[36m(TaskRunner pid=5690)[0m                                    'pipeline_model_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                    'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=5690)[0m                                                 'all_ranks': False,
[36m(TaskRunner pid=5690)[0m                                                 'enable': False,
[36m(TaskRunner pid=5690)[0m                                                 'ranks': [],
[36m(TaskRunner pid=5690)[0m                                                 'save_path': 'outputs/profile',
[36m(TaskRunner pid=5690)[0m                                                 'tool': None,
[36m(TaskRunner pid=5690)[0m                                                 'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                         'analysis': True,
[36m(TaskRunner pid=5690)[0m                                                                         'contents': [],
[36m(TaskRunner pid=5690)[0m                                                                         'discrete': False,
[36m(TaskRunner pid=5690)[0m                                                                         'level': 'level0'},
[36m(TaskRunner pid=5690)[0m                                                                 'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                          'discrete': False},
[36m(TaskRunner pid=5690)[0m                                                                 'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                           'step_end': None,
[36m(TaskRunner pid=5690)[0m                                                                           'step_start': 0},
[36m(TaskRunner pid=5690)[0m                                                                 'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                                  'stack_depth': 32,
[36m(TaskRunner pid=5690)[0m                                                                                  'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=5690)[0m                                    'prometheus': {'_target_': 'verl.workers.config.PrometheusConfig',
[36m(TaskRunner pid=5690)[0m                                                   'enable': False,
[36m(TaskRunner pid=5690)[0m                                                   'file': '/tmp/ray/session_latest/metrics/prometheus/prometheus.yml',
[36m(TaskRunner pid=5690)[0m                                                   'port': 9090,
[36m(TaskRunner pid=5690)[0m                                                   'served_model_name': 'Qwen/Qwen2.5-3B-Instruct'},
[36m(TaskRunner pid=5690)[0m                                    'prompt_length': 2048,
[36m(TaskRunner pid=5690)[0m                                    'quantization': None,
[36m(TaskRunner pid=5690)[0m                                    'quantization_config_file': None,
[36m(TaskRunner pid=5690)[0m                                    'response_length': 4096,
[36m(TaskRunner pid=5690)[0m                                    'skip_dump_dir': '/tmp/rollout_dump',
[36m(TaskRunner pid=5690)[0m                                    'skip_rollout': False,
[36m(TaskRunner pid=5690)[0m                                    'skip_tokenizer_init': True,
[36m(TaskRunner pid=5690)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=5690)[0m                                    'tensor_model_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                    'top_k': -1,
[36m(TaskRunner pid=5690)[0m                                    'top_p': 1.0,
[36m(TaskRunner pid=5690)[0m                                    'trace': {'_target_': 'verl.workers.config.TraceConfig',
[36m(TaskRunner pid=5690)[0m                                              'backend': None,
[36m(TaskRunner pid=5690)[0m                                              'max_samples_per_step_per_worker': None,
[36m(TaskRunner pid=5690)[0m                                              'token2text': False},
[36m(TaskRunner pid=5690)[0m                                    'update_weights_bucket_megabytes': 512,
[36m(TaskRunner pid=5690)[0m                                    'val_kwargs': {'_target_': 'verl.workers.config.SamplingConfig',
[36m(TaskRunner pid=5690)[0m                                                   'do_sample': True,
[36m(TaskRunner pid=5690)[0m                                                   'n': 1,
[36m(TaskRunner pid=5690)[0m                                                   'temperature': 1.0,
[36m(TaskRunner pid=5690)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=5690)[0m                                                   'top_p': 0.7}}},
[36m(TaskRunner pid=5690)[0m  'algorithm': {'_target_': 'verl.trainer.config.AlgoConfig',
[36m(TaskRunner pid=5690)[0m                'adv_estimator': 'grpo',
[36m(TaskRunner pid=5690)[0m                'gamma': 1.0,
[36m(TaskRunner pid=5690)[0m                'kl_ctrl': {'_target_': 'verl.trainer.config.KLControlConfig',
[36m(TaskRunner pid=5690)[0m                            'horizon': 10000,
[36m(TaskRunner pid=5690)[0m                            'kl_coef': 0.0,
[36m(TaskRunner pid=5690)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=5690)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=5690)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=5690)[0m                'lam': 1.0,
[36m(TaskRunner pid=5690)[0m                'norm_adv_by_std_in_grpo': True,
[36m(TaskRunner pid=5690)[0m                'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0},
[36m(TaskRunner pid=5690)[0m                'rollout_correction': {'bypass_mode': False,
[36m(TaskRunner pid=5690)[0m                                       'loss_type': 'ppo_clip',
[36m(TaskRunner pid=5690)[0m                                       'rollout_is': None,
[36m(TaskRunner pid=5690)[0m                                       'rollout_is_batch_normalize': False,
[36m(TaskRunner pid=5690)[0m                                       'rollout_is_threshold': 2.0,
[36m(TaskRunner pid=5690)[0m                                       'rollout_rs': None,
[36m(TaskRunner pid=5690)[0m                                       'rollout_rs_threshold': None,
[36m(TaskRunner pid=5690)[0m                                       'rollout_rs_threshold_lower': None,
[36m(TaskRunner pid=5690)[0m                                       'rollout_token_veto_threshold': None},
[36m(TaskRunner pid=5690)[0m                'use_kl_in_reward': False,
[36m(TaskRunner pid=5690)[0m                'use_pf_ppo': False},
[36m(TaskRunner pid=5690)[0m  'critic': {'_target_': 'verl.workers.config.FSDPCriticConfig',
[36m(TaskRunner pid=5690)[0m             'checkpoint': {'_target_': 'verl.trainer.config.CheckpointConfig',
[36m(TaskRunner pid=5690)[0m                            'async_save': False,
[36m(TaskRunner pid=5690)[0m                            'load_contents': ['model', 'optimizer', 'extra'],
[36m(TaskRunner pid=5690)[0m                            'save_contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=5690)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=5690)[0m             'data_loader_seed': 42,
[36m(TaskRunner pid=5690)[0m             'enable': None,
[36m(TaskRunner pid=5690)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=5690)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=5690)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=5690)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=5690)[0m             'loss_agg_mode': 'seq-mean-token-mean',
[36m(TaskRunner pid=5690)[0m             'model': {'_target_': 'verl.workers.config.FSDPCriticModelCfg',
[36m(TaskRunner pid=5690)[0m                       'enable_activation_offload': False,
[36m(TaskRunner pid=5690)[0m                       'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=5690)[0m                       'external_lib': None,
[36m(TaskRunner pid=5690)[0m                       'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=5690)[0m                                       'dtype': 'bfloat16',
[36m(TaskRunner pid=5690)[0m                                       'entropy_checkpointing': False,
[36m(TaskRunner pid=5690)[0m                                       'entropy_from_logits_with_chunking': False,
[36m(TaskRunner pid=5690)[0m                                       'forward_only': False,
[36m(TaskRunner pid=5690)[0m                                       'forward_prefetch': False,
[36m(TaskRunner pid=5690)[0m                                       'fsdp_size': -1,
[36m(TaskRunner pid=5690)[0m                                       'full_determinism': False,
[36m(TaskRunner pid=5690)[0m                                       'model_dtype': 'fp32',
[36m(TaskRunner pid=5690)[0m                                       'offload_policy': False,
[36m(TaskRunner pid=5690)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=5690)[0m                                       'param_offload': False,
[36m(TaskRunner pid=5690)[0m                                       'reshard_after_forward': True,
[36m(TaskRunner pid=5690)[0m                                       'seed': 42,
[36m(TaskRunner pid=5690)[0m                                       'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m                                       'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                                       'use_orig_params': False,
[36m(TaskRunner pid=5690)[0m                                       'use_torch_compile': True,
[36m(TaskRunner pid=5690)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=5690)[0m                       'lora_alpha': 16,
[36m(TaskRunner pid=5690)[0m                       'lora_rank': 0,
[36m(TaskRunner pid=5690)[0m                       'override_config': {},
[36m(TaskRunner pid=5690)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=5690)[0m                       'target_modules': 'all-linear',
[36m(TaskRunner pid=5690)[0m                       'tokenizer_path': 'Qwen/Qwen2.5-3B-Instruct',
[36m(TaskRunner pid=5690)[0m                       'trust_remote_code': False,
[36m(TaskRunner pid=5690)[0m                       'use_remove_padding': False,
[36m(TaskRunner pid=5690)[0m                       'use_shm': False},
[36m(TaskRunner pid=5690)[0m             'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig',
[36m(TaskRunner pid=5690)[0m                       'betas': [0.9, 0.999],
[36m(TaskRunner pid=5690)[0m                       'clip_grad': 1.0,
[36m(TaskRunner pid=5690)[0m                       'lr': 1e-05,
[36m(TaskRunner pid=5690)[0m                       'lr_scheduler_type': 'constant',
[36m(TaskRunner pid=5690)[0m                       'lr_warmup_steps': -1,
[36m(TaskRunner pid=5690)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=5690)[0m                       'min_lr_ratio': 0.0,
[36m(TaskRunner pid=5690)[0m                       'num_cycles': 0.5,
[36m(TaskRunner pid=5690)[0m                       'optimizer': 'AdamW',
[36m(TaskRunner pid=5690)[0m                       'optimizer_impl': 'torch.optim',
[36m(TaskRunner pid=5690)[0m                       'override_optimizer_config': None,
[36m(TaskRunner pid=5690)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=5690)[0m                       'warmup_style': None,
[36m(TaskRunner pid=5690)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=5690)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=5690)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=5690)[0m             'ppo_micro_batch_size': 
[36m(TaskRunner pid=5690)[0m None
[36m(TaskRunner pid=5690)[0m ,
[36m(TaskRunner pid=5690)[0m             'ppo_micro_batch_size_per_gpu'
[36m(TaskRunner pid=5690)[0m : 
[36m(TaskRunner pid=5690)[0m None
[36m(TaskRunner pid=5690)[0m ,
[36m(TaskRunner pid=5690)[0m             
[36m(TaskRunner pid=5690)[0m 'ppo_mini_batch_size'
[36m(TaskRunner pid=5690)[0m : 
[36m(TaskRunner pid=5690)[0m 16,
[36m(TaskRunner pid=5690)[0m             'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=5690)[0m                          'all_ranks': False,
[36m(TaskRunner pid=5690)[0m                          'enable': False,
[36m(TaskRunner pid=5690)[0m                          'ranks': [],
[36m(TaskRunner pid=5690)[0m                          'save_path': 'outputs/profile',
[36m(TaskRunner pid=5690)[0m                          'tool': None,
[36m(TaskRunner pid=5690)[0m                          'tool_config': {'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=5690)[0m                                                  'analysis': True,
[36m(TaskRunner pid=5690)[0m                                                  'contents': [],
[36m(TaskRunner pid=5690)[0m                                                  'discrete': False,
[36m(TaskRunner pid=5690)[0m                                                  'level': 'level0'},
[36m(TaskRunner pid=5690)[0m                                          'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=5690)[0m                                                   'discrete': False},
[36m(TaskRunner pid=5690)[0m                                          'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=5690)[0m                                                    'step_end': None,
[36m(TaskRunner pid=5690)[0m                                                    'step_start': 0},
[36m(TaskRunner pid=5690)[0m                                          'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=5690)[0m                                                           'stack_depth': 32,
[36m(TaskRunner pid=5690)[0m                                                           'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=5690)[0m             'rollout_n': 4,
[36m(TaskRunner pid=5690)[0m             'shuffle': False,
[36m(TaskRunner pid=5690)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m             'use_dynamic_bsz': True},
[36m(TaskRunner pid=5690)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=5690)[0m  'data': {'apply_chat_template_kwargs': {},
[36m(TaskRunner pid=5690)[0m           'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=5690)[0m           'datagen': {'name': None, 'path': None},
[36m(TaskRunner pid=5690)[0m           'dataloader_num_workers': 8,
[36m(TaskRunner pid=5690)[0m           'filter_overlong_prompts': True,
[36m(TaskRunner pid=5690)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=5690)[0m           'image_key': 'images',
[36m(TaskRunner pid=5690)[0m           'image_patch_size': 14,
[36m(TaskRunner pid=5690)[0m           'max_prompt_length': 2048,
[36m(TaskRunner pid=5690)[0m           'max_response_length': 4096,
[36m(TaskRunner pid=5690)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=5690)[0m           'return_full_prompt': False,
[36m(TaskRunner pid=5690)[0m           'return_multi_modal_inputs': True,
[36m(TaskRunner pid=5690)[0m           'return_raw_chat': True,
[36m(TaskRunner pid=5690)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=5690)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=5690)[0m           'sampler': {'class_name': None, 'class_path': None},
[36m(TaskRunner pid=5690)[0m           'seed': None,
[36m(TaskRunner pid=5690)[0m           'shuffle': True,
[36m(TaskRunner pid=5690)[0m           'tokenizer': None,
[36m(TaskRunner pid=5690)[0m           'tool_config_path': None,
[36m(TaskRunner pid=5690)[0m           'train_batch_size': 64,
[36m(TaskRunner pid=5690)[0m           'train_files': ['/workspace/verl/data/gsm8k/train.parquet'],
[36m(TaskRunner pid=5690)[0m           'train_max_samples': -1,
[36m(TaskRunner pid=5690)[0m           'truncation': 'error',
[36m(TaskRunner pid=5690)[0m           'trust_remote_code': False,
[36m(TaskRunner pid=5690)[0m           'use_shm': False,
[36m(TaskRunner pid=5690)[0m           'val_batch_size': None,
[36m(TaskRunner pid=5690)[0m           'val_files': ['/workspace/verl/data/gsm8k/test.parquet'],
[36m(TaskRunner pid=5690)[0m           'val_max_samples': -1,
[36m(TaskRunner pid=5690)[0m           'validation_shuffle': False,
[36m(TaskRunner pid=5690)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=5690)[0m  'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=5690)[0m                      'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=5690)[0m                                                      'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=5690)[0m                                                                                    'cuda-memory-usage': 'true',
[36m(TaskRunner pid=5690)[0m                                                                                    'trace': 'cuda,nvtx,cublas,ucx'},
[36m(TaskRunner pid=5690)[0m                                                      'discrete': False,
[36m(TaskRunner pid=5690)[0m                                                      'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(TaskRunner pid=5690)[0m                                                                                'capture-range-end': None,
[36m(TaskRunner pid=5690)[0m                                                                                'cuda-graph-trace': 'graph',
[36m(TaskRunner pid=5690)[0m                                                                                'cuda-memory-usage': 'true',
[36m(TaskRunner pid=5690)[0m                                                                                'kill': 'none',
[36m(TaskRunner pid=5690)[0m                                                                                'trace': 'cuda,nvtx,cublas,ucx'}},
[36m(TaskRunner pid=5690)[0m                                             'torch_memory': {'context': 'all',
[36m(TaskRunner pid=5690)[0m                                                              'kw_args': {},
[36m(TaskRunner pid=5690)[0m                                                              'stack_depth': 32,
[36m(TaskRunner pid=5690)[0m                                                              'stacks': 'all',
[36m(TaskRunner pid=5690)[0m                                                              'trace_alloc_max_entries': 100000}},
[36m(TaskRunner pid=5690)[0m                      'profile_continuous_steps': False,
[36m(TaskRunner pid=5690)[0m                      'save_path': 'outputs/profile',
[36m(TaskRunner pid=5690)[0m                      'steps': None,
[36m(TaskRunner pid=5690)[0m                      'tool': None},
[36m(TaskRunner pid=5690)[0m  'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None},
[36m(TaskRunner pid=5690)[0m  'reward_manager': {'_target_': 'verl.trainer.config.config.RewardManagerConfig',
[36m(TaskRunner pid=5690)[0m                     'module': {'_target_': 'verl.trainer.config.config.ModuleConfig',
[36m(TaskRunner pid=5690)[0m                                'name': 'custom_reward_manager',
[36m(TaskRunner pid=5690)[0m                                'path': None},
[36m(TaskRunner pid=5690)[0m                     'name': 'dapo',
[36m(TaskRunner pid=5690)[0m                     'source': 'register'},
[36m(TaskRunner pid=5690)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=5690)[0m                   'enable_resource_pool': False,
[36m(TaskRunner pid=5690)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=5690)[0m                   'launch_reward_fn_async': False,
[36m(TaskRunner pid=5690)[0m                   'max_length': None,
[36m(TaskRunner pid=5690)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=5690)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=5690)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=5690)[0m                             'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(TaskRunner pid=5690)[0m                                             'forward_prefetch': False,
[36m(TaskRunner pid=5690)[0m                                             'fsdp_size': -1,
[36m(TaskRunner pid=5690)[0m                                             'param_offload': False,
[36m(TaskRunner pid=5690)[0m                                             'reshard_after_forward': True,
[36m(TaskRunner pid=5690)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=5690)[0m                             'input_tokenizer': 'Qwen/Qwen2.5-3B-Instruct',
[36m(TaskRunner pid=5690)[0m                             'override_config': {},
[36m(TaskRunner pid=5690)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=5690)[0m                             'trust_remote_code': False,
[36m(TaskRunner pid=5690)[0m                             'use_fused_kernels': False,
[36m(TaskRunner pid=5690)[0m                             'use_remove_padding': False,
[36m(TaskRunner pid=5690)[0m                             'use_shm': False},
[36m(TaskRunner pid=5690)[0m                   'n_gpus_per_node': 8,
[36m(TaskRunner pid=5690)[0m                   'nnodes': 0,
[36m(TaskRunner pid=5690)[0m                   'num_workers': 1,
[36m(TaskRunner pid=5690)[0m                   'profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(TaskRunner pid=5690)[0m                                'all_ranks': False,
[36m(TaskRunner pid=5690)[0m                                'enable': False,
[36m(TaskRunner pid=5690)[0m                                'ranks': [],
[36m(TaskRunner pid=5690)[0m                                'save_path': 'outputs/profile',
[36m(TaskRunner pid=5690)[0m                                'tool': None,
[36m(TaskRunner pid=5690)[0m                                'tool_config': {
[36m(TaskRunner pid=5690)[0m 'npu': {'_target_': 'verl.utils.profiler.config.NPUToolConfig',
[36m(TaskRunner pid=5690)[0m                                                        'analysis': True,
[36m(TaskRunner pid=5690)[0m                                                        'contents': [],
[36m(TaskRunner pid=5690)[0m                                                        'discrete': False,
[36m(TaskRunner pid=5690)[0m                                                        'level': 'level0'},
[36m(TaskRunner pid=5690)[0m                                                'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(TaskRunner pid=5690)[0m                                                         'discrete': False},
[36m(TaskRunner pid=5690)[0m                                                'torch': {'_target_': 'verl.utils.profiler.config.TorchProfilerToolConfig',
[36m(TaskRunner pid=5690)[0m                                                          'step_end': None,
[36m(TaskRunner pid=5690)[0m                                                          'step_start': 0},
[36m(TaskRunner pid=5690)[0m                                                'torch_memory': {'_target_': 'verl.utils.profiler.config.TorchMemoryToolConfig',
[36m(TaskRunner pid=5690)[0m                                                                 'stack_depth': 32,
[36m(TaskRunner pid=5690)[0m                                                                 'trace_alloc_max_entries': 100000}}},
[36m(TaskRunner pid=5690)[0m                   'reward_kwargs': {'max_resp_len': 4096,
[36m(TaskRunner pid=5690)[0m                                     'overlong_buffer_cfg': {'enable': False,[36m(TaskRunner pid=5690)[0m /workspace/verl/verl/verl/trainer/main_ppo.py:307: UserWarning: Disabled critic as algorithm.adv_estimator != gae. If it is not intended, please set critic.enable=True
[36m(TaskRunner pid=5690)[0m   use_critic=need_critic(config),
[36m(TaskRunner pid=5690)[0m /workspace/verl/verl/verl/utils/profiler/config.py:49: UserWarning: Torch profiler tool config is not fully supported now.
[36m(TaskRunner pid=5690)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(TaskRunner pid=5690)[0m Generating train split: 0 examples [00:00, ? examples/s]
[36m(TaskRunner pid=5690)[0m Generating train split: 7473 examples [00:00, 264415.13 examples/s]
[36m(TaskRunner pid=5690)[0m Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=5690)[0m WARNING:2025-12-24 04:58:43,091:Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):   0%|          | 0/7473 [00:00<?, ? examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  13%|â–ˆâ–Ž        | 1000/7473 [00:00<00:05, 1266.04 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 2000/7473 [00:01<00:02, 2116.18 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3000/7473 [00:01<00:01, 2709.22 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 4000/7473 [00:01<00:01, 3165.13 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 5000/7473 [00:01<00:00, 3478.03 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 6000/7473 [00:01<00:00, 3754.46 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 7000/7473 [00:02<00:00, 3945.54 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:02<00:00, 4021.71 examples/s]Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7473/7473 [00:02<00:00, 3120.26 examples/s]
[36m(TaskRunner pid=5690)[0m Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1319 examples [00:00, 160491.05 examples/s]
[36m(TaskRunner pid=5690)[0m Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=5690)[0m WARNING:2025-12-24 04:58:45,645:Setting TOKENIZERS_PARALLELISM=false for forked processes.
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):   0%|          | 0/1319 [00:00<?, ? examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1000/1319 [00:00<00:00, 1388.96 examples/s]
[36m(TaskRunner pid=5690)[0m Filtering prompts longer than 2048 tokens (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [00:00<00:00, 1489.84 examples/s]
[36m(TaskRunner pid=5690)[0m /workspace/verl/verl/verl/trainer/ppo/ray_trainer.py:335: UserWarning: Disabled critic as algorithm.adv_estimator != gae. If it is not intended, please set critic.enable=True
[36m(TaskRunner pid=5690)[0m   self.use_critic = need_critic(self.config)

[36m(TaskRunner pid=5690)[0m                                                             'len': 4096,
[36m(TaskRunner pid=5690)[0m                                                             'log': False,
[36m(TaskRunner pid=5690)[0m                                                             'penalty_factor': 1.0}},
[36m(TaskRunner pid=5690)[0m                   'reward_loop_class_name': None,
[36m(TaskRunner pid=5690)[0m                   'reward_loop_module_path': None,
[36m(TaskRunner pid=5690)[0m                   'reward_loop_source': 'register',
[36m(TaskRunner pid=5690)[0m                   'reward_manager': 'dapo',
[36m(TaskRunner pid=5690)[0m                   'rollout': {'_target_': 'verl.workers.config.RolloutConfig',
[36m(TaskRunner pid=5690)[0m                               'cudagraph_capture_sizes': None,
[36m(TaskRunner pid=5690)[0m                               'data_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                               'disable_log_stats': True,
[36m(TaskRunner pid=5690)[0m                               'dtype': 'bfloat16',
[36m(TaskRunner pid=5690)[0m                               'enable_chunked_prefill': True,
[36m(TaskRunner pid=5690)[0m                               'enable_prefix_caching': True,
[36m(TaskRunner pid=5690)[0m                               'enforce_eager': True,
[36m(TaskRunner pid=5690)[0m                               'engine_kwargs': {},
[36m(TaskRunner pid=5690)[0m                               'expert_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                               'free_cache_engine': True,
[36m(TaskRunner pid=5690)[0m                               'gpu_memory_utilization': 0.5,
[36m(TaskRunner pid=5690)[0m                               'limit_images': None,
[36m(TaskRunner pid=5690)[0m                               'load_format': 'auto',
[36m(TaskRunner pid=5690)[0m                               'max_model_len': None,
[36m(TaskRunner pid=5690)[0m                               'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=5690)[0m                               'max_num_seqs': 1024,
[36m(TaskRunner pid=5690)[0m                               'name': '???',
[36m(TaskRunner pid=5690)[0m                               'prompt_length': 2048,
[36m(TaskRunner pid=5690)[0m                               'response_length': 2048,
[36m(TaskRunner pid=5690)[0m                               'skip_tokenizer_init': False,
[36m(TaskRunner pid=5690)[0m                               'tensor_model_parallel_size': 2},
[36m(TaskRunner pid=5690)[0m                   'sandbox_fusion': {'max_concurrent': 64,
[36m(TaskRunner pid=5690)[0m                                      'memory_limit_mb': 1024,
[36m(TaskRunner pid=5690)[0m                                      'url': None},
[36m(TaskRunner pid=5690)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=5690)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=5690)[0m                   'use_dynamic_bsz': True,
[36m(TaskRunner pid=5690)[0m                   'use_reward_loop': True},
[36m(TaskRunner pid=5690)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=5690)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=5690)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=5690)[0m              'default_local_dir': '/workspace/verl/checkpoints/gspo/gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL',
[36m(TaskRunner pid=5690)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=5690)[0m              'device': 'cuda',
[36m(TaskRunner pid=5690)[0m              'esi_redundant_time': 0,
[36m(TaskRunner pid=5690)[0m              'experiment_name': 'gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL',
[36m(TaskRunner pid=5690)[0m              'log_val_generations': 2,
[36m(TaskRunner pid=5690)[0m              'logger': ['console'],
[36m(TaskRunner pid=5690)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=5690)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=5690)[0m              'n_gpus_per_node': 1,
[36m(TaskRunner pid=5690)[0m              'nnodes': 1,
[36m(TaskRunner pid=5690)[0m              'project_name': 'RL-GSPO',
[36m(TaskRunner pid=5690)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=5690)[0m              'resume_from_path': None,
[36m(TaskRunner pid=5690)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=5690)[0m              'rollout_data_dir': None,
[36m(TaskRunner pid=5690)[0m              'save_freq': 10,
[36m(TaskRunner pid=5690)[0m              'test_freq': 10,
[36m(TaskRunner pid=5690)[0m              'total_epochs': 1,
[36m(TaskRunner pid=5690)[0m              'total_training_steps': 100,
[36m(TaskRunner pid=5690)[0m              'use_legacy_worker_impl': 'auto',
[36m(TaskRunner pid=5690)[0m              'val_before_train': False,
[36m(TaskRunner pid=5690)[0m              'val_only': False,
[36m(TaskRunner pid=5690)[0m              'validation_data_dir': None},
[36m(TaskRunner pid=5690)[0m  'transfer_queue': {'enable': False}}
[36m(TaskRunner pid=5690)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=5690)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=5690)[0m dataset len: 7473
[36m(TaskRunner pid=5690)[0m filter dataset len: 7473
[36m(TaskRunner pid=5690)[0m Using dataset class: RLHFDataset
[36m(TaskRunner pid=5690)[0m dataset len: 1319
[36m(TaskRunner pid=5690)[0m filter dataset len: 1319
[36m(TaskRunner pid=5690)[0m Size of train dataloader: 116, Size of val dataloader: 1
[36m(TaskRunner pid=5690)[0m Total training steps: 100
[36m(TaskRunner pid=5690)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(TaskRunner pid=5690)[0m bind role actor_rollout method chat_completion to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=5690)[0m bind role actor_rollout method generate to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=5690)[0m bind role actor_rollout method get_zeromq_address to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=5690)[0m bind role actor_rollout method sleep to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(TaskRunner pid=5690)[0m bind role actor_rollout method wake_up to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=6053)[0m   "architectures": [
[36m(WorkerDict pid=6053)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=6053)[0m   ],
[36m(WorkerDict pid=6053)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=6053)[0m   "attn_implementation": "sdpa",
[36m(WorkerDict pid=6053)[0m   "dtype": "bfloat16",
[36m(WorkerDict pid=6053)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=6053)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=6053)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=6053)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=6053)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=6053)[0m   "layer_types": [
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",[36m(WorkerDict pid=6053)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(WorkerDict pid=6053)[0m Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=6053)[0m Fetching 2 files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.51s/it]Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.76s/it]
[36m(WorkerDict pid=6053)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=6053)[0m Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.03s/it]
[36m(WorkerDict pid=6053)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.35s/it]
[36m(WorkerDict pid=6053)[0m /usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_init_utils.py:430: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=6053)[0m   warnings.warn(
[36m(WorkerDict pid=6053)[0m /workspace/verl/verl/verl/utils/profiler/config.py:49: UserWarning: Torch profiler tool config is not fully supported now.
[36m(WorkerDict pid=6053)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(WorkerDict pid=6053)[0m /usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=6053)[0m   warnings.warn(
[36m(pid=6430)[0m W1224 04:59:28.016000 6430 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
[36m(TaskRunner pid=5690)[0m /workspace/verl/verl/verl/utils/profiler/config.py:49: UserWarning: Torch profiler tool config is not fully supported now.
[36m(TaskRunner pid=5690)[0m   warnings.warn("Torch profiler tool config is not fully supported now.", stacklevel=1)
[36m(vLLMHttpServer pid=6430)[0m INFO:2025-12-24 04:59:28,980:vLLMHttpServer, replica_rank: 0, master address: 162.243.206.134, master port: 42105, data parallel master port: 34679
[36m(vLLMHttpServer pid=6430)[0m INFO:2025-12-24 04:59:28,984:override_generation_config: {'temperature': 1.0, 'top_k': -1, 'top_p': 1.0, 'repetition_penalty': 1.0, 'max_new_tokens': 4096}
[36m(vLLMHttpServer pid=6430)[0m INFO:2025-12-24 04:59:28,984:enable_sleep_mode: True
[36m(vLLMHttpServer pid=6430)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(vLLMHttpServer pid=6430)[0m INFO:2025-12-24 04:59:29,001:replica_rank=0, node_rank=0, nnodes=1, get worker zmq addresses: ['ipc:///tmp/verl_vllm_zmq_6053_root.ipc']
[36m(vLLMHttpServer pid=6430)[0m `torch_dtype` is deprecated! Use `dtype` instead!
[36m(vLLMHttpServer pid=6430)[0m W1224 04:59:39.748000 6506 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
[36m(WorkerDict pid=6053)[0m 2025-12-24 05:00:04,965 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[36m(WorkerDict pid=6053)[0m 2025-12-24 05:00:05,016 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 28.76it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–ˆ         | 7/67 [00:00<00:02, 28.27it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 10/67 [00:00<00:02, 27.84it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–‰        | 13/67 [00:00<00:01, 28.47it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 16/67 [00:00<00:01, 28.89it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 19/67 [00:00<00:01, 28.69it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22/67 [00:00<00:01, 28.88it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:01, 28.12it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/67 [00:00<00:01, 28.45it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/67 [00:01<00:01, 27.98it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:01<00:01, 27.27it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/67 [00:01<00:01, 26.71it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:01<00:01, 26.04it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:01<00:00, 25.32it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/67 [00:01<00:00, 24.67it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:01<00:00, 23.74it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 52/67 [00:01<00:00, 23.64it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:02<00:00, 23.59it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 58/67 [00:02<00:00, 23.74it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:02<00:00, 23.74it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:02<00:00, 23.96it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.97it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/67 [00:00<?, ?it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 4/67 [00:00<00:01, 37.41it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  12%|â–ˆâ–        | 8/67 [00:00<00:01, 34.99it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  19%|â–ˆâ–‰        | 13/67 [00:00<00:01, 37.34it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:00<00:01, 38.20it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 38.26it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:00<00:01, 38.67it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:00<00:00, 38.99it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:00<00:00, 38.83it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/67 [00:00<00:00, 38.06it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/67 [00:01<00:00, 37.42it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 37.58it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:01<00:00, 36.73it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 53/67 [00:01<00:00, 36.99it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:01<00:00, 36.93it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:01<00:00, 37.40it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:01<00:00, 38.11it/s]
[36m(WorkerDict pid=6053)[0m Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:01<00:00, 37.79it/s]
[36m(vLLMHttpServer pid=6430)[0m INFO:2025-12-24 05:00:10,703:Initializing a V1 LLM engine with config: model='Qwen/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=dummy, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[36m(TaskRunner pid=5690)[0m Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]
[36m(pid=6824)[0m W1224 05:00:16.892000 6824 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
[36m(AgentLoopWorker pid=6824)[0m You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[36m(pid=7790)[0m W1224 05:00:24.291000 7790 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=6053)[0m /usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
[36m(WorkerDict pid=6053)[0m   return fn(*args, **kwargs)
[36m(WorkerDict pid=6053)[0m /usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
[36m(WorkerDict pid=6053)[0m   warnings.warn(
[36m(AgentLoopWorker pid=6821)[0m You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.[32m [repeated 7x across cluster][0m
[36m(pid=8095)[0m W1224 05:00:24.944000 8095 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'[32m [repeated 7x across cluster][0m

[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention",
[36m(WorkerDict pid=6053)[0m     "full_attention"
[36m(WorkerDict pid=6053)[0m   ],
[36m(WorkerDict pid=6053)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=6053)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=6053)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=6053)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=6053)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=6053)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=6053)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=6053)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=6053)[0m   "rope_scaling": null,
[36m(WorkerDict pid=6053)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=6053)[0m   "sliding_window": null,
[36m(WorkerDict pid=6053)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=6053)[0m   "transformers_version": "4.57.1",
[36m(WorkerDict pid=6053)[0m   "use_cache": true,
[36m(WorkerDict pid=6053)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=6053)[0m   "vocab_size": 151936
[36m(WorkerDict pid=6053)[0m }
[36m(WorkerDict pid=6053)[0m 
[36m(WorkerDict pid=6053)[0m Monkey patch state_dict in AutoModelForCausalLMWithValueHead. 
[36m(WorkerDict pid=6053)[0m Skipping monkey patch for Qwen2ForCausalLM as use_fused_kernels is False or fused_kernels_backend is torch
[36m(WorkerDict pid=6053)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=6053)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fb8d83c7240>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fb8d83c7100>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=6053)[0m Total steps: 100, num_warmup_steps: 5
[36m(WorkerDict pid=6053)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=6053)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=6053)[0m NCCL version 2.27.3+cuda12.9
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(TaskRunner pid=5690)[0m WARNING 12-24 04:59:21 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(pid=6430)[0m WARNING 12-24 04:59:27 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(vLLMHttpServer pid=6430)[0m ['serve',
[36m(vLLMHttpServer pid=6430)[0m  'Qwen/Qwen2.5-3B-Instruct',
[36m(vLLMHttpServer pid=6430)[0m  '--dtype',
[36m(vLLMHttpServer pid=6430)[0m  'bfloat16',
[36m(vLLMHttpServer pid=6430)[0m  '--load_format',
[36m(vLLMHttpServer pid=6430)[0m  'dummy',
[36m(vLLMHttpServer pid=6430)[0m  '--max_model_len',
[36m(vLLMHttpServer pid=6430)[0m  '6144',
[36m(vLLMHttpServer pid=6430)[0m  '--max_num_seqs',
[36m(vLLMHttpServer pid=6430)[0m  '1024',
[36m(vLLMHttpServer pid=6430)[0m  '--enable_chunked_prefill',
[36m(vLLMHttpServer pid=6430)[0m  '--max_num_batched_tokens',
[36m(vLLMHttpServer pid=6430)[0m  '6144',
[36m(vLLMHttpServer pid=6430)[0m  '--enable_prefix_caching',
[36m(vLLMHttpServer pid=6430)[0m  '--enable_sleep_mode',
[36m(vLLMHttpServer pid=6430)[0m  '--disable_custom_all_reduce',
[36m(vLLMHttpServer pid=6430)[0m  '--gpu_memory_utilization',
[36m(vLLMHttpServer pid=6430)[0m  '0.6',
[36m(vLLMHttpServer pid=6430)[0m  '--disable_log_stats',
[36m(vLLMHttpServer pid=6430)[0m  '--tensor_parallel_size',
[36m(vLLMHttpServer pid=6430)[0m  '1',
[36m(vLLMHttpServer pid=6430)[0m  '--seed',
[36m(vLLMHttpServer pid=6430)[0m  '0',
[36m(vLLMHttpServer pid=6430)[0m  '--override_generation_config',
[36m(vLLMHttpServer pid=6430)[0m  '{"temperature": 1.0, "top_k": -1, "top_p": 1.0, "repetition_penalty": 1.0, '
[36m(vLLMHttpServer pid=6430)[0m  '"max_new_tokens": 4096}',
[36m(vLLMHttpServer pid=6430)[0m  '--hf_overrides',
[36m(vLLMHttpServer pid=6430)[0m  '{}']
[36m(vLLMHttpServer pid=6430)[0m WARNING 12-24 04:59:33 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: In a Ray actor and can only be spawned
[36m(vLLMHttpServer pid=6430)[0m WARNING 12-24 04:59:39 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(WorkerDict pid=6053)[0m [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[36m(vLLMHttpServer pid=6430)[0m WARNING 12-24 05:00:10 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(WorkerDict pid=6053)[0m WARNING 12-24 04:59:39 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
[36m(TaskRunner pid=5690)[0m AgentLoopManager: ['162.243.206.134:36117']
[36m(TaskRunner pid=5690)[0m Checkpoint tracker file does not exist: /workspace/verl/checkpoints/gspo/gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=5690)[0m Training from scratch
[36m(AgentLoopWorker pid=6824)[0m WARNING 12-24 05:00:17 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development!
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'actor_rollout_ref.actor.policy_loss.loss_mode=gspo', '+actor_rollout_ref.model.override_config.attn_implementation=sdpa', '+actor_rollout_ref.ref.override_config.attn_implementation=sdpa', "data.train_files=['/workspace/verl/data/gsm8k/train.parquet']", "data.val_files=['/workspace/verl/data/gsm8k/test.parquet']", 'data.shuffle=true', 'data.prompt_key=prompt', 'data.truncation=error', 'data.filter_overlong_prompts=true', 'data.return_raw_chat=True', 'data.train_batch_size=64', 'data.max_prompt_length=2048', 'data.max_response_length=4096', 'actor_rollout_ref.rollout.n=4', 'algorithm.use_kl_in_reward=false', 'algorithm.kl_ctrl.kl_coef=0.0', 'actor_rollout_ref.actor.use_kl_loss=false', 'actor_rollout_ref.actor.kl_loss_coef=0.0', 'actor_rollout_ref.actor.clip_ratio_low=0.0003', 'actor_rollout_ref.actor.clip_ratio_high=0.0004', 'actor_rollout_ref.model.use_remove_padding=false', 'actor_rollout_ref.actor.use_dynamic_bsz=true', 'actor_rollout_ref.ref.log_prob_use_dynamic_bsz=true', 'actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=true', 'actor_rollout_ref.actor.ppo_max_token_len_per_gpu=12288', 'actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=18432', 'actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=18432', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.mode=async', 'actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct', 'actor_rollout_ref.model.enable_gradient_checkpointing=true', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.05', 'actor_rollout_ref.actor.optim.weight_decay=0.1', 'actor_rollout_ref.actor.ppo_mini_batch_size=16', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2', 'actor_rollout_ref.actor.fsdp_config.param_offload=false', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=false', 'actor_rollout_ref.actor.entropy_coeff=0', 'actor_rollout_ref.actor.grad_clip=1.0', 'actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-mean', 'actor_rollout_ref.actor.ulysses_sequence_parallel_size=1', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.enable_chunked_prefill=true', 'actor_rollout_ref.rollout.max_num_batched_tokens=6144', 'actor_rollout_ref.rollout.temperature=1.0', 'actor_rollout_ref.rollout.top_p=1.0', 'actor_rollout_ref.rollout.top_k=-1', 'actor_rollout_ref.rollout.val_kwargs.temperature=1.0', 'actor_rollout_ref.rollout.val_kwargs.top_p=0.7', 'actor_rollout_ref.rollout.val_kwargs.top_k=-1', 'actor_rollout_ref.rollout.val_kwargs.do_sample=true', 'actor_rollout_ref.rollout.val_kwargs.n=1', 'actor_rollout_ref.ref.fsdp_config.param_offload=false', 'actor_rollout_ref.ref.ulysses_sequence_parallel_size=1', 'actor_rollout_ref.actor.entropy_checkpointing=true', 'reward_model.reward_manager=dapo', '+reward_model.reward_kwargs.overlong_buffer_cfg.enable=false', '+reward_model.reward_kwargs.overlong_buffer_cfg.len=4096', '+reward_model.reward_kwargs.overlong_buffer_cfg.penalty_factor=1.0', '+reward_model.reward_kwargs.overlong_buffer_cfg.log=false', '+reward_model.reward_kwargs.max_resp_len=4096', 'trainer.logger=["console"]', 'trainer.project_name=RL-GSPO', 'trainer.experiment_name=gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL', 'trainer.n_gpus_per_node=1', 'trainer.nnodes=1', 'trainer.val_before_train=false', 'trainer.test_freq=10', 'trainer.save_freq=10', 'trainer.total_epochs=1', 'trainer.total_training_steps=100', 'trainer.default_local_dir=/workspace/verl/checkpoints/gspo/gspo-epslow-0.0003-epshigh-0.0004-Qwen2.5-3B-Instruct-RL', 'trainer.resume_mode=auto', 'trainer.log_val_generations=2']
[36m(TaskRunner pid=5690)[0m Training Progress:   0%|          | 0/100 [00:23<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/verl/verl/verl/trainer/main_ppo.py", line 45, in main
    run_ppo(config)
  File "/workspace/verl/verl/verl/trainer/main_ppo.py", line 99, in run_ppo
    ray.get(runner.run.remote(config))
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 2882, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py", line 968, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=5690, ip=162.243.206.134, actor_id=4ff6977580fa5a27a0d9817701000000, repr=<main_ppo.TaskRunner object at 0x7f591bb6b650>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/trainer/main_ppo.py", line 374, in run
    trainer.fit()
  File "/workspace/verl/verl/verl/trainer/ppo/ray_trainer.py", line 1436, in fit
    old_log_prob, old_log_prob_mfu = self._compute_old_log_prob(batch)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/trainer/ppo/ray_trainer.py", line 1183, in _compute_old_log_prob
    old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/single_controller/ray/base.py", line 53, in __call__
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=6053, ip=162.243.206.134, actor_id=33879ebca60cac97b76ae20201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fb5baf48bf0>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/single_controller/ray/base.py", line 841, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/single_controller/base/decorator.py", line 456, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/utils/transferqueue_utils.py", line 309, in dummy_inner
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/utils/profiler/profile.py", line 256, in wrapper
    return func(self_instance, *args, **kwargs_inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/workers/fsdp_workers.py", line 981, in compute_log_prob
    output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/workers/actor/dp_actor.py", line 378, in compute_log_prob
    entropy, log_probs = self._forward_micro_batch(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/workers/actor/dp_actor.py", line 305, in _forward_micro_batch
    entropy = torch.utils.checkpoint.checkpoint(verl_F.entropy_from_logits, logits)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py", line 488, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 576, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/workspace/verl/verl/verl/utils/torch_functional.py", line 147, in entropy_from_logits
    pd = torch.nn.functional.softmax(logits, dim=-1)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 2137, in softmax
    ret = input.softmax(dim)
          ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 99.69 GiB. GPU 0 has a total capacity of 139.80 GiB of which 50.93 GiB is free. Process 52582 has 88.87 GiB memory in use. Of the allocated memory 168.40 GiB is allocated by PyTorch, with 283.41 MiB allocated in private pools (e.g., CUDA Graphs), and 103.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(AgentLoopWorker pid=6821)[0m WARNING 12-24 05:00:18 [api_server.py:1213] LoRA dynamic loading & unloading is enabled in the API server. This should ONLY be used for local development![32m [repeated 7x across cluster][0m
