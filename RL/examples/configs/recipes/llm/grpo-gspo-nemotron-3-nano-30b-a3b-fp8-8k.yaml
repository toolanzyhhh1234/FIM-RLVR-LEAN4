defaults: ../../grpo_math_1B.yaml

grpo:
  num_prompts_per_step: 128
  num_generations_per_prompt: 8
  max_val_samples: 480
  val_batch_size: 32

loss_fn:
  reference_policy_kl_penalty: 0.0
  sequence_level_importance_ratios: true
  token_level_loss: false
  use_importance_sampling_correction: true

checkpointing:
  keep_top_k: 10
  model_save_format: null
  checkpoint_dir: results/grpo-gspo-nemotron-3-nano-30b-a3b-fp8-8k

policy:
  model_name: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8
  tokenizer:
    name: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8
  train_global_batch_size: 64
  train_micro_batch_size: 1
  max_total_sequence_length: 8192
  dtensor_cfg:
    enabled: false
  sequence_packing:
    enabled: false
  optimizer:
    kwargs:
      lr: 2.0e-06
  make_sequence_length_divisible_by: ${policy.megatron_cfg.tensor_model_parallel_size}
  megatron_cfg:
    enabled: true
    tensor_model_parallel_size: 4
    pipeline_model_parallel_size: 2
    expert_model_parallel_size: 2
    sequence_parallel: true
    activation_checkpointing: true
    fp8_cfg:
      enabled: true
      fp8: e4m3
      fp8_recipe: blockwise
      fp8_param: false
    env_vars:
      NVTE_FP8_BLOCK_SCALING_FP32_SCALES: '1'
  generation:
    vllm_cfg:
      precision: fp8
      enforce_eager: true
      max_model_len: 8192
      tensor_parallel_size: ${policy.megatron_cfg.tensor_model_parallel_size}

data:
  dataset_name: DeepScaler

env:
  math:
    num_workers: 16

logger:
  monitor_gpus: false

cluster:
  gpus_per_node: 8
  num_nodes: 1
